"use strict";(self.webpackChunkonboarding_baania=self.webpackChunkonboarding_baania||[]).push([[978],{3905:function(e,n,t){t.d(n,{Zo:function(){return c},kt:function(){return u}});var a=t(7294);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function i(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,a,r=function(e,n){if(null==e)return{};var t,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var l=a.createContext({}),p=function(e){var n=a.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):i(i({},n),e)),t},c=function(e){var n=p(e.components);return a.createElement(l.Provider,{value:n},e.children)},d={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},m=a.forwardRef((function(e,n){var t=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),m=p(t),u=r,f=m["".concat(l,".").concat(u)]||m[u]||d[u]||o;return t?a.createElement(f,i(i({ref:n},c),{},{components:t})):a.createElement(f,i({ref:n},c))}));function u(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var o=t.length,i=new Array(o);i[0]=m;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,i[1]=s;for(var p=2;p<o;p++)i[p]=t[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,t)}m.displayName="MDXCreateElement"},6367:function(e,n,t){t.r(n),t.d(n,{assets:function(){return c},contentTitle:function(){return l},default:function(){return u},frontMatter:function(){return s},metadata:function(){return p},toc:function(){return d}});var a=t(7462),r=t(3366),o=(t(7294),t(3905)),i=["components"],s={title:"PySpark"},l=void 0,p={unversionedId:"docs/data-science/pyspark",id:"docs/data-science/pyspark",title:"PySpark",description:"Install",source:"@site/docs/docs/data-science/pyspark.md",sourceDirName:"docs/data-science",slug:"/docs/data-science/pyspark",permalink:"/docs/data-science/pyspark",draft:!1,editUrl:"https://github.com/baania/onboarding.baania.com/edit/master/docs/docs/data-science/pyspark.md",tags:[],version:"current",frontMatter:{title:"PySpark"},sidebar:"docs",previous:{title:"PostGIS",permalink:"/docs/data-science/postgis"},next:{title:"Intro",permalink:"/docs/git/intro"}},c={},d=[{value:"Install",id:"install",level:2},{value:"Resources",id:"resources",level:2},{value:"Test snippet",id:"test-snippet",level:3},{value:"Init",id:"init",level:2},{value:"Add JARs at runtine",id:"add-jars-at-runtine",level:3},{value:"I/O",id:"io",level:2},{value:"DataFrame",id:"dataframe",level:2},{value:"Transformations",id:"transformations",level:2},{value:"functions",id:"functions",level:3},{value:"datetime",id:"datetime",level:3},{value:"Cookbook",id:"cookbook",level:2},{value:"Count missing values  + groupby",id:"count-missing-values---groupby",level:3},{value:"Filter by order",id:"filter-by-order",level:3},{value:"Optimization",id:"optimization",level:2},{value:"Caching",id:"caching",level:3},{value:"Repartition + partition data",id:"repartition--partition-data",level:3},{value:"Dynamic partition write mode",id:"dynamic-partition-write-mode",level:3},{value:"Skew join optimization",id:"skew-join-optimization",level:3},{value:"JDBC",id:"jdbc",level:2},{value:"Postgres",id:"postgres",level:3},{value:"MongoDB",id:"mongodb",level:3},{value:"spark-submit",id:"spark-submit",level:2},{value:"Misc",id:"misc",level:2}],m={toc:d};function u(e){var n=e.components,t=(0,r.Z)(e,i);return(0,o.kt)("wrapper",(0,a.Z)({},m,t,{components:n,mdxType:"MDXLayout"}),(0,o.kt)("h2",{id:"install"},"Install"),(0,o.kt)("admonition",{type:"info"},(0,o.kt)("p",{parentName:"admonition"},"make sure pyspark version is same as spark version")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"brew install apache-spark\npip3 install pyspark\n")),(0,o.kt)("h2",{id:"resources"},"Resources"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://github.com/palantir/pyspark-style-guide"},"PySpark Style Guide"))),(0,o.kt)("h3",{id:"test-snippet"},"Test snippet"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from pyspark import SparkContext, SparkConf\nimport numpy as np\nfrom operator import itemgetter\nfrom matplotlib import pyplot as plt\n\n\nsc = SparkContext.getOrCreate()\n\nTOTAL = 100000\ndots = sc.parallelize([2.0 * np.random.random(2) - 1.0 for i in range(TOTAL)]).cache()\nprint("Number of random points:", dots.count())\n\nstats = dots.stats()\nprint("Mean:", stats.mean())\nprint("stdev:", stats.stdev())\n\nplt.figure(figsize=(10, 5))\n\n# Plot 1\nplt.subplot(1, 2, 1)\nplt.xlim((-1.0, 1.0))\nplt.ylim((-1.0, 1.0))\n\nsample = dots.sample(False, 0.01)\nX = sample.map(itemgetter(0)).collect()\nY = sample.map(itemgetter(1)).collect()\nplt.scatter(X, Y)\n\n# Plot 2\nplt.subplot(1, 2, 2)\nplt.xlim((-1.0, 1.0))\nplt.ylim((-1.0, 1.0))\n\ninCircle = lambda v: np.linalg.norm(v) <= 1.0\ndotsIn = sample.filter(inCircle).cache()\ndotsOut = sample.filter(lambda v: not inCircle(v)).cache()\n\n# inside circle\nXin = dotsIn.map(itemgetter(0)).collect()\nYin = dotsIn.map(itemgetter(1)).collect()\nplt.scatter(Xin, Yin, color="r")\n\n# outside circle\nXout = dotsOut.map(itemgetter(0)).collect()\nYout = dotsOut.map(itemgetter(1)).collect()\nplt.scatter(Xout, Yout)\n')),(0,o.kt)("h2",{id:"init"},"Init"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, when, lit, coalesce\nfrom pyspark.sql.types import (\n    StructType,\n    StructField,\n    IntegerType,\n    DoubleType,\n    StringType,\n    TimestampType,\n)\nfrom pyspark.sql.window import Window\nimport pyspark.sql.functions as F\n\nspark = (\n    SparkSession.builder.appName("Pyspark playground")\n    .config("spark.hadoop.fs.s3a.access.key", KEY)\n    .config("spark.hadoop.fs.s3a.secret.key", SECRET)\n    .config("spark.executor.memory", "4g")\n    .config("spark.driver.memory", "4g")\n    .getOrCreate()\n)\n\n# set config after spark session is created\nspark_session.conf.set("spark.executor.memory", "8g")\n\nspark.sparkContext.setLogLevel("ERROR")\nspark.sparkContext.setCheckpointDir("checkpoint")  # [DEBUG]\n')),(0,o.kt)("h3",{id:"add-jars-at-runtine"},"Add JARs at runtine"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"import os\n\nos.environ['PYSPARK_SUBMIT_ARGS'] = '--packages \"org.apache.hadoop:hadoop-aws:2.7.3\" pyspark-shell'\n")),(0,o.kt)("h2",{id:"io"},"I/O"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# CSV / TSV\nproject = spark.read.csv(\n    project_file,\n    header='true',\n    inferSchema='true',\n    sep=\"\\t\",\n    nullValue=r'\\N',\n    timestampFormat=\"yyyy-MM-dd HH:mm:ss\"\n)\nspark.write.csv(output_path, header=True)\n\n# JSON\nspark.read.json(\"data/DMP_HIVE/all_listing.json\") # add .option(\"multiLine\", True) for multi-line\nspark.write.json(OUTPATH, compression='gzip')\n")),(0,o.kt)("h2",{id:"dataframe"},"DataFrame"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'# describe dataframe\ndf.printSchema()  # .columns(), .dtypes(), describe() also exists\n\n# select columns\ndf.select(["a", "b", "c"])\n\n# sampling\ndf.sample(False, sampling_percentage, seed=0)\n\n# count records\ndf.count()\n\n# conversions\ndf.toPandas()  # spark to pandas\nspark_session.createDataFrame(df)  # pandas to spark\n\n# show in vertical\ndf.show(n=3, truncate=False, vertical=True)\n')),(0,o.kt)("h2",{id:"transformations"},"Transformations"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'# rename columns\ndf.withColumnRenamed("old_name", "new_name")\n\n# add null column\ndf.withColumn(col_name, lit(None).cast(col_type))\n\n# dtype casting\ndf.withColumn("col_name", df["col_name"].cast(IntegerType()))\n\n# combine values from multiple rows via groupby\ndf.groupBy(groupby_col).agg(F.collect_list(col_name))\n\n# select elem by name from array column\nF.col(col_name)["$elem_key"])\n\n## by index\nF.col(col_name).getItem(0)\n\n# find median\ndf_spark.approxQuantile(df_spark.columns, [0.5], 0.25)\n\n# get percentile\ndf.approxQuantile(["Apple", "Oranges"], [0.1, 0.25, 0.5, 0.75, 0.9, 0.95], 0.1)\n\n# join\ndf.join(\n    df2,\n    [\n        key\n    ],  # df.key == df2.key in case keys are different, otherwise [COL_NAME] to prevent column duplicates\n    how="left",\n)\n')),(0,o.kt)("h3",{id:"functions"},"functions"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'# combine cols to array\nF.array("x_1", "x_2"))\n\n# fillna with another column\nF.coalesce("a", "b")\n\n# create new column with max value from set of columns\nF.greatest(a["one"], a["two"], a["three"])\n\n# regex matching --\x3e longest maching works if longest regex is at the start\nF.regexp_replace(trim(lower(col(col_name))), regex_str, "")\n\n# explode array\ndf.withColumn("tmp", F.explode("tmp")).select(\n    *df.columns,\n    col("tmp.a"),\n    col("tmp.b"),\n    col("tmp.c")\n)\n\n# convert to JSON\nF.to_json(c)\n')),(0,o.kt)("h3",{id:"datetime"},"datetime"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'# epoch to timestamp\nF.from_unixtime(df.column / 1000)\n\n# timestamp to date\nF.to_date("listing_update")\n\n# utz to to tz\nF.from_utc_timestamp("datetime_utc", "CST")\n')),(0,o.kt)("h2",{id:"cookbook"},"Cookbook"),(0,o.kt)("h3",{id:"count-missing-values---groupby"},"Count missing values  + groupby"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns])\n")),(0,o.kt)("h3",{id:"filter-by-order"},"Filter by order"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from pyspark.sql.window import Window\n\n\nw = Window().partitionBy(partition_col).orderBy(F.desc(order_by_key))\n\n(\n    df\n    .withColumn("rank", F.row_number().over(w))\n    .filter(col("rank") == 1)\n    .drop(col("rank"))\n)\n\n')),(0,o.kt)("h2",{id:"optimization"},"Optimization"),(0,o.kt)("h3",{id:"caching"},"Caching"),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://stackoverflow.com/questions/26870537/what-is-the-difference-between-cache-and-persist"},"More details")),(0,o.kt)("admonition",{type:"info"},(0,o.kt)("p",{parentName:"admonition"},"Improves read performance for frequently accessed DataFrame")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"df.cache()\n\n# clear cache\nspark.catalog.clearCache()\n")),(0,o.kt)("h3",{id:"repartition--partition-data"},"Repartition + partition data"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"df.repartition(4)\ndf.write.partitionBy(*partition_columns).parquet(base_path, mode=write_mode)\n")),(0,o.kt)("h3",{id:"dynamic-partition-write-mode"},"Dynamic partition write mode"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'"""\nNote: Why do we have to change partitionOverwriteMode?\nWithout config partitionOverwriteMode = \'dynamic\', Spark will\noverwrite all partitions in hierarchy with the new\ndata we are writing. That\'s undesirable and dangerous.\nhttps://stackoverflow.com/questions/42317738/how-to-partition-and-write-dataframe-in-spark-without-deleting-partitions-with-n\nTherefore, we will temporarily use \'dynamic\' within the context of writing files to storage.\n"""\n')),(0,o.kt)("h3",{id:"skew-join-optimization"},"Skew join optimization"),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://stackoverflow.com/a/57951114"},"https://stackoverflow.com/a/57951114")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from math import exp\nfrom random import randint\nfrom datetime import datetime\n\n\ndef count_elements(splitIndex, iterator):\n    n = sum(1 for _ in iterator)\n    yield (splitIndex, n)\n\n\ndef get_part_index(splitIndex, iterator):\n    for it in iterator:\n        yield (splitIndex, it)\n\n\nnum_parts = 18\n# create the large skewed rdd\nskewed_large_rdd = sc.parallelize(range(0, num_parts), num_parts).flatMap(\n    lambda x: range(0, int(exp(x)))\n)\nskewed_large_rdd = skewed_large_rdd.mapPartitionsWithIndex(\n    lambda ind, x: get_part_index(ind, x)\n)\nskewed_large_df = spark.createDataFrame(skewed_large_rdd, ["x", "y"])\n\nsmall_rdd = sc.parallelize(range(0, num_parts), num_parts).map(lambda x: (x, x))\nsmall_df = spark.createDataFrame(small_rdd, ["a", "b"])\n\n## prep salts\nsalt_bins = 100\nfrom pyspark.sql import functions as F\n\nskewed_transformed_df = skewed_large_df.withColumn(\n    "salt", (F.rand() * salt_bins).cast("int")\n).cache()\n\nsmall_transformed_df = small_df.withColumn(\n    "replicate", F.array([F.lit(i) for i in range(salt_bins)])\n)\nsmall_transformed_df = (\n    small_transformed_df.select("*", F.explode("replicate").alias("salt"))\n    .drop("replicate")\n    .cache()\n)\n\n## magic happens here\nt0 = datetime.now()\nresult2 = skewed_transformed_df.join(\n    small_transformed_df,\n    (skewed_transformed_df["x"] == small_transformed_df["a"])\n    & (skewed_transformed_df["salt"] == small_transformed_df["salt"]),\n)\nresult2.count()\nprint("The direct join takes %s" % (str(datetime.now() - t0)))\n')),(0,o.kt)("h2",{id:"jdbc"},"JDBC"),(0,o.kt)("admonition",{type:"info"},(0,o.kt)("p",{parentName:"admonition"},"To overwrite without losing schema & permission, use ",(0,o.kt)("inlineCode",{parentName:"p"},"truncate"))),(0,o.kt)("h3",{id:"postgres"},"Postgres"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'(\n    spark.read.format("jdbc")\n    .option("url", "jdbc:postgresql://{}:5432/{}".format(host, db_name))\n    .option("query", query)\n    .option("user", username)\n    .option("password", password)\n    .option("driver", "org.postgresql.Driver")\n    .load()\n)\n\n(\n    df.write.format("jdbc")\n    .option("url", "jdbc:postgresql://{}:5432/{}".format(host, db_name))\n    .option("dbtable", table_name)\n    .option("user", username)\n    .option("password", password)\n    .option("driver", "org.postgresql.Driver")\n    .option("truncate", "true")\n    .mode("overwrite")\n    .save()\n)\n')),(0,o.kt)("h3",{id:"mongodb"},"MongoDB"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'import os\n\nos.environ[\n    "PYSPARK_SUBMIT_ARGS"\n] = \'--packages "org.mongodb.spark:mongo-spark-connector_2.11:2.4.2" pyspark-shell\'\n\nfrom pyspark.sql import SparkSession\n\nspark = (\n    SparkSession.builder.appName("myApp")\n    .config(\n        "spark.mongodb.input.uri",\n        "mongodb://USER:PASSWORD@HOST:27017/DB.COLLECTION?authSource=admin&readPreference=primary&ssl=false",\n    )\n    .config(\n        "spark.mongodb.input.uri",\n        "mongodb://USER:PASSWORD@HOST:27017/DB.COLLECTION?authSource=admin&readPreference=primary&ssl=false",\n    )\n    .getOrCreate()\n)\n')),(0,o.kt)("h2",{id:"spark-submit"},"spark-submit"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},'spark-submit --conf spark.driver.memory=25gb --executor-memory 13g --num-executors 50 --driver-memory 20g FILE.PY\n\nspark-submit --conf maximizeResourceAllocation=true FILE.PY\n\n### AWS EMR\n# also change checkpoint dir to "mnt/checkpoint"\nspark-submit --py-files dist-matrix-module.zip  property_distance_matrix.py\n\n# alternative\nspark-submit --deploy-mode cluster s3://<PATH TO FILE>/sparky.py\n')),(0,o.kt)("h2",{id:"misc"},"Misc"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"# get spark location\necho 'sc.getConf.get(\"spark.home\")' | spark-shell\n")))}u.isMDXComponent=!0}}]);